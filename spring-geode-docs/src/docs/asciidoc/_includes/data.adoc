[[geode-data-using]]
== Using Data
:geode-name: Apache Geode

One of the most important tasks during development is ensuring your Spring Boot application handles data correctly.
In order to verify the accuracy, integrity and availability of your data, your application needs data to work with.

For those already familiar with Spring Boot's support for {spring-boot-docs-html}/howto.html#howto-initialize-a-database-using-spring-jdbc[SQL database initialization],
the approach when using {geode-name} should be easy to understand.

{geode-name} provides built-in support, similar in function to Spring Boot's SQL database initialization, by using:

* _Gfsh's_ {apache-geode-docs}/tools_modules/gfsh/quick_ref_commands_by_area.html#topic_C7DB8A800D6244AE8FF3ADDCF139DCE4[import/export] data commands.
* {apache-geode-docs}/managing/cache_snapshots/chapter_overview.html[Snapshot Service]
* {apache-geode-docs}/developing/storing_data_on_disk/chapter_overview.html[Persistence] with {apache-geode-docs}/managing/disk_storage/chapter_overview.html[Disk Storage]

For example, by enabling Persistence with Disk Storage, you could {apache-geode-docs}/managing/disk_storage/backup_restore_disk_store.html[backup and restore]
persistent `DiskStore` files from one cluster to another.

Alternatively, using {geode-name}'s _Snapshot Service_, you can export data contained in targeted `Regions` from one
cluster during shutdown and import the data into another cluster on startup. The _Snapshot Service_ allows you to filter
data while its being imported and exported.

Finally, GemFire/Geode Shell (_Gfsh_) commands can be used to {spring-data-geode-docs-html}/tools_modules/gfsh/command-pages/export.html#topic_263B70069BFC4A7185F86B3272011734[export data]
and {apache-geode-docs}/tools_modules/gfsh/command-pages/import.html#topic_jw2_2ld_2l[import data].

TIP: Spring Data for Apache Geode (SDG) contains dedicated support for {spring-data-geode-docs-html}/#bootstrap:region:persistence[Persistence]
and the {spring-data-geode-docs-html}/#bootstrap:snapshot[Snapshot Service].

In all cases, the files generated by _persistence_, the _Snapshot Service_ and _Gfsh's_ `export` command are in a
proprietary, binary format.

Furthermore, none of these approaches are as convenient as Spring Boot's database initialization automation. Therefore,
Spring Boot for Apache Geode (SBDG) offers support to import data from JSON into {geode-name} as PDX.

Unlike Spring Boot, SBDG offers support to export data as well. Data is imported and exported in JSON format, by default.

NOTE: SBDG does not provide an equivalent to Spring Boot's `schema.sql` file. The best way to define the data structures
(i.e. `Regions`) managing your data is with SDG's Annotation-based configuration support for defining cache `Regions`
from your application's {spring-data-geode-docs-html}/#bootstrap-annotation-config-regions[entity classes] or indirectly
from Spring and JSR-107, JCache {spring-data-geode-docs-html}/#bootstrap-annotation-config-caching[caching annotations].

TIP: Refer to SBDG's <<geode-configuration-declarative-annotations-productivity-regions,documentation>> on the same.

WARNING: While this feature has utility and many edge cases were thought through and tested thoroughly, there are still
some limitations that need to be ironed out. See https://github.com/spring-projects/spring-boot-data-geode/issues/82[Issue-82]
and https://github.com/spring-projects/spring-boot-data-geode/issues/83[Issue-83] for more details. The Spring team
strongly recommends that this feature only be used for development and testing purposes.

[[geode-data-using-import]]
=== Importing Data

You can import data into a `Region` by defining a JSON file containing the JSON object(s) you wish to load. The JSON
file must follow the naming convention below and be placed in the root of your application classpath:

`data-<regionName>.json`

NOTE: `<regionName>` refers to the lowercase "name" of the `Region` as defined by
{apache-geode-javadoc}/org/apache/geode/cache/Region.html#getName--[Region.getName()].

For example, if you have a `Region` named "_Orders_", then you would create a JSON file called `data-orders.json`
and place it in the root of your application classpath (e.g. in `src/test/resources`).

Create JSON files for each `Region` implicitly defined (e.g. by using `@EnableEntityDefinedRegions`) or explicitly
defined (i.e. with `ClientRegionFactoryBean` in _JavaConfig_) in your Spring Boot application configuration that you
want to load with data.

The JSON file containing JSON data for _Orders_ might appear as follows:

.`data-orders.json`
[source,json]
----
[{
  "@type": "example.app.pos.model.PurchaseOrder",
  "id": 1,
  "lineItems": [
    {
      "@type": "example.app.pos.model.LineItem",
      "product": {
        "@type": "example.app.pos.model.Product",
        "name": "Apple iPad Pro",
        "price": 1499.00,
        "category": "SHOPPING"
      },
      "quantity": 1
    },
    {
      "@type": "example.app.pos.model.LineItem",
      "product": {
        "@type": "example.app.pos.model.Product",
        "name": "Apple iPhone 11 Pro Max",
        "price": 1249.00,
        "category": "SHOPPING"
      },
      "quantity": 2
    }
  ]
}, {
  "@type": "example.app.pos.model.PurchaseOrder",
  "id": 2,
  "lineItems": [
    {
      "@type": "example.app.pos.model.LineItem",
      "product": {
        "@type": "example.app.pos.model.Product",
        "name": "Starbucks Vente Carmel Macchiato",
        "price": 5.49,
        "category": "SHOPPING"
      },
      "quantity": 1
    }
  ]
}]
----

The application entity classes matching the JSON data might look something like:

.Point-of-Sale (POS) Application Model Classes
[source,java]
----
@Region("Orders")
class PurchaseOrder {

	@Id
    Long id;

	List<LineItem> lineItems;

}

class LineItem {

	Product product;
	Integer quantity;

}

@Region("Products")
class Product {

	String name;
	Category category;
	BigDecimal price;

}
----

As seen above, the object model and corresponding JSON can be arbitrarily complex with a hierarchy of objects
having complex types.

[[geode-data-using-import-metadata]]
==== JSON metadata

You will notice a few other details contained in the object model and JSON shown above.

[[geode-data-using-import-metadata-attype]]
===== The `@type` metadata field

First, we declared an `@type` JSON metadata field.  This field does not map to any specific field or property of
the application domain model class (e.g. `PurchaseOrder`). Rather, it tells the framework and {geode-name}'s JSON/PDX
converter the type of object the JSON data would map to if you were to request an object (i.e. by calling
`PdxInstance.getObject()`).

For example:

.Deserializing PDX as an Object
[source,java]
----
@Repository
class OrdersRepository {

    @Resource(name = "Orders")
    Region<Long, PurchaseOrder> orders;

    PurchaseOrder findBy(Long id) {

        Object value = this.orders.get(id);

        return value instanceof PurchaseOrder ? (PurchaseOrder) value
            : value instanceof PdxInstance ? ((PdxInstance) value).getObject()
            : null;
    }
}
----

Basically, the `@type` JSON metadata field informs the `PdxInstance.getObject()` method about the type of Java object
the JSON object will map to. Otherwise, the `PdxInstance.getObject()` method would silently return a `PdxInstance`.

It is possible for {geode-name}'s PDX serialization framework to return a `PurchaseOrder` from `Region.get(key)` as well,
but it depends on the value of PDX's `read-serialized`, cache-level configuration setting, among other factors.

NOTE: When JSON is imported into a `Region` as PDX, the {apache-geode-javadoc}/org/apache/geode/pdx/PdxInstance.html#getClassName--[PdxInstance.getClassName()]
does not refer to a valid Java class. It is {apache-geode-javadoc}/org/apache/geode/pdx/JSONFormatter.html#JSON_CLASSNAME[JSONFormatter.JSON_CLASSNAME].
As a result, `Region` data access operations, such as `Region.get(key)`, return a `PdxInstance` and not a Java object.

TIP: You may need to proxy `Region` "read" data access operations (e.g. `Region.get(key)`) by setting the SBDG property
`spring.boot.data.gemfire.cache.region.advice.enabled` to `true`. When this property is set, `Regions` are proxied to
wrap a `PdxInstance` in a `PdxInstanceWrapper` in order to appropriately handle the `PdxInstance.getObject()` call in
your application code.

[[geode-data-using-import-metadata-id]]
===== The `id` field & `@identifier` metadata field

The top-level objects in your JSON must have an identifier, such as an "id" field.  This identifier is used as the
object's (or `PdxInstance`'s) identity and "key" when stored in the `Region` (e.g. `Region.put(key, object)`).

You will have noticed the the JSON for the _Orders_ above declared an "id" field as the identifier:

.PurchaseOrder identifier ("id")
[source,text]
----
[{
  "@type": "example.app.pos.model.PurchaseOrder",
  "id": 1,
  ...
----

This follows the same convention used in Spring Data.  Typically, Spring Data mapping infrastructure looks for a POJO
field or property annotated with {spring-data-commons-javadoc}/org/springframework/data/annotation/Id.html[@Id]. If no
field or property is annotated with `@Id`, then the framework falls back to searching for a field or property named "id".

In Spring Data for Apache Geode (SDG), this `@Id` annotated, or "id" named field or property is used as the identifier,
and as the key for the object when storing it into a `Region`.

However, what happens when an object, or entity does not have a surrogate id defined?  Perhaps the application domain
model class is appropriately and simply using "natural" identifiers, which is quite common in practice.

Consider a `Book` class defined as follows:

.Book class
[source,java]
----
@Region("Books")
class Book {

	Author author;

	@Id
	ISBN isbn;

	LocalDate publishedDate;

	Sring title;

}
----

As declared in the `Book` class above, the identifier for `Book` is its `ISBN` since the `isbn` field was annotated with
Spring Data's `@Id` mapping annotation. However, we cannot know this by searching for an `@Id` annotation in JSON.

You might be tempted to argue that if the `@type` metadata field is set, we would know the class type and could load
the class definition to learn about the identifier. That is all fine until the class is not actually on the application
classpath in the first place. This is one of the reasons why SBDG's JSON support serializes JSON to {geode-name}'s PDX
format. There might not be a class definition, which would lead to a `NoClassDefFoundError` or `ClassNotFoundException`.

So, what then?

In this case, SBDG allows you to declare the `@identifier` JSON metadata field to inform the framework
what to use as the identifier for the object.

For example:

.Using "@identifer"
[source,json]
----
{
  "@type": "example.app.books.model.Book",
  "@identifier": "isbn",
  "author": {
    "id": 1,
    "name": "Josh Long"
  },
  "isbn": "978-1-449-374640-8",
  "publishedDate": "2017-08-01",
  "title": "Cloud Native Java"
}
----

Here, the `@identifier` JSON metadata field informs the framework that the "isbn" field is the identifier for a `Book`.

[[geode-data-using-import-conditional]]
==== Conditionally Importing Data

While the Spring team recommends that users should only use this feature when developing and testing their Spring Boot
applications with Apache Geode, a user may occasionally use this feature in production.

Users might use this feature in production to preload a (REPLICATE) Region with "reference" data. Reference data is
largely static, infrequently changing and non-transactional. Preloading reference data is particularly useful in caching
use cases, where you want to "warm" the cache.

When using this feature for development and testing purposes, you can simply put your `Region` specific JSON files in
`src/test/resources`. This ensures the files will not be included in your application artifact (e.g. JAR, WAR) when
deployed to production.

However, if you must use this feature to preload data in your production environment, then you can still "conditionally"
load data from JSON. To do so configure the `spring.boot.data.gemfire.cache.data.import.active-profiles` property set to
the Spring profile(s) that must be active for the import to take effect.

For example:

.Conditional Importing JSON
[source,properties]
----
# Spring Boot application.properties

spring.boot.data.gemfire.cache.data.import.active-profiles=DEV, QA
----

In order for import to have an effect in this example, you must specifically set the `spring.profiles.active`
property to 1 of the valid, "_active-profiles_" listed in the import property (e.g. `QA`). Only 1 needs to match.

NOTE: There are many ways to conditionally build application artifacts. Some users might prefer to handle this concern
in their Gradle or Maven builds.

[[geode-data-using-export]]
=== Exporting Data

Certain data stored in your application's `Regions` may be sensitive or confidential and keeping the data secure is of
the utmost concern and priority. Therefore, exporting data is **disabled** by default.

However, if you are using this feature for development and testing purposes then enabling the _export_ capability may be
useful to move data from 1 environment to another. For example, if your QA team finds a bug in the application using a
particular data set, then they can _export_ the data and pass it back to the development team to _import_ in their local
development environment to help debug the issue.

To enable _export_, set the `spring.boot.data.gemfire.cache.data.export.enabled` property to `true`:

.Enable Export
[source,properties]
----
# Spring Boot application.properties

spring.boot.data.gemfire.cache.data.export.enabled=true
----

SBDG is careful to _export_ data to JSON in a format that {geode-name} expects on _import_ and includes things such as
`@type` metadata fields.

WARNING: The `@identifier` metadata field is not generated automatically. While it is possible for POJOs stored in a
`Region` to include an `@identifier` metadata field when exported to JSON it is not possible when the `Region` value
is a `PdxInstance` that did not originate from JSON. In this case, you must manually ensure the `PdxInstance` includes
an `@identifier` metadata field before it is exported to JSON if necessary (e.g. `Book.isbn`). This is only necessary
if your entity classes do not declare an explicit identifier field, such as with the `@Id` mapping annotation, or do
not have an "id" field. This scenario can also occur when inter-operating with native clients that model the application
domain objects differently, then serialize the objects using PDX storing them in Regions on the server that are then
later consumed by your Spring Boot application.
